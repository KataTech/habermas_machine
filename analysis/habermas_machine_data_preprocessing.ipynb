{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZKXwtiiBiL"
      },
      "source": [
        "# Habermas Machine data and preprocessing\n",
        "\n",
        "The purpose of this colab is to demonstrate the loading of data from Google Cloud Storage and the basic preprocessing steps that were performed on the data used in the paper:\n",
        "\n",
        "Tessler, M. H., Bakker, M. A., Jarrett D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, J., Collins, T., Parkes, D. C., Botvinick, M., and Summerfield, C. \"AI can help humans find common ground in democratic deliberation.\" *Science*. (2024)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-21EkbgjiDtF"
      },
      "source": [
        "# Setup and Importing Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bou7ACLh6Yj"
      },
      "outputs": [],
      "source": [
        "# Clone github repo locally.\n",
        "!git clone https://github.com/google-deepmind/habermas_machine\n",
        "\n",
        "# Adjust path.\n",
        "import sys\n",
        "sys.path.insert(0,'/content/habermas_machine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdDwJRN2iGtW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import ast\n",
        "import io\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Local imports\n",
        "from analysis import live_loading, serialise, types\n",
        "\n",
        "# Load helper keys used with dataframes.\n",
        "DFKeys = serialise.SerialisedComparisonKeys\n",
        "DFGroupedKeys = serialise.GroupedSerialisedComparisonKeys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDRqfi3MiI5x"
      },
      "outputs": [],
      "source": [
        "#@title Load all comparison data from Google Cloud Storage.\n",
        "comparison_data_location = (\n",
        "    'https://storage.googleapis.com/habermas_machine/datasets/hm_all_candidate_comparisons.parquet'\n",
        ")\n",
        "response = requests.get(comparison_data_location)\n",
        "with io.BytesIO(response.content) as f:\n",
        "  df_all = pd.read_parquet(f)\n",
        "clear_output()\n",
        "\n",
        "df_all.shape # Shape of full comparison data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdBIn4jgiL7z"
      },
      "outputs": [],
      "source": [
        "#@title Explore data\n",
        "\n",
        "print(\"Number of participant sessions (before pre-processing):\",\n",
        "      df_all[DFKeys.COMPARISON_PARTICIPANT_ID].nunique())\n",
        "\n",
        "print(\n",
        "    \"Number of participant sessions (before pre-processing) of each collection:\",\n",
        "    df_all[\n",
        "        [DFKeys.COMPARISON_VERSION, DFKeys.COMPARISON_PARTICIPANT_ID]\n",
        "    ].drop_duplicates().groupby(DFKeys.COMPARISON_VERSION).count()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WlV_Ye1iOwa"
      },
      "source": [
        "# Example preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJjl6SLZiNYy"
      },
      "outputs": [],
      "source": [
        "#@title Select dataset\n",
        "\n",
        "dataset_name = 'training' # @param [\"training\", \"cohort1_ablation_iid_v1\", \"cohort2_ablation_iid_v2\", \"cohort3_ablation_ood_v1\", 'cohort4_critique_exclusion', 'cohort5_opinion_exposure', 'cohort6_human_mediator', 'virtual_citizens_assembly']\n",
        "\n",
        "# Set dataset and parameters based on dataset_name.\n",
        "if dataset_name == 'training':\n",
        "  df = df_all[\n",
        "      df_all[DFKeys.COMPARISON_VERSION].isin([\n",
        "          'TRAINING_DATA_V1',\n",
        "          'TRAINING_DATA_V2',\n",
        "          'TRAINING_DATA_V3',\n",
        "          'TRAINING_DATA_V4',\n",
        "          'TRAINING_DATA_V5',\n",
        "      ])\n",
        "  ]\n",
        "  # Backwards incompatibility issue with training data.\n",
        "  df = df.drop(columns=[\n",
        "      DFKeys.CANDIDATES_ALL_REWARD_DATA_WELFARE_OR_RANK,\n",
        "      DFKeys.CANDIDATES_REWARD_DATA_WELFARE_OR_RANK,\n",
        "  ])\n",
        "  min_size_parameters = None\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort1_ablation_iid_v1':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT1_ABLATION_IID_V1']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V1\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort2_ablation_iid_v2':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT2_ABLATION_IID_V2']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V2\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort3_ablation_ood_v1':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT3_ABLATION_OOD_V1']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_OOD_V1\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort4_critique_exclusion':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT4_CRITIQUE_EXCLUSION']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_CRITIQUE_EXCLUSION\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort5_opinion_exposure':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT5_OPINION_EXPOSURE']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_OPINION_EXPOSURE\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (\n",
        "      types.ResponseProvenance.HUMAN_CITIZEN, # Candidates are other opinions.\n",
        "  )\n",
        "elif dataset_name == 'cohort6_human_mediator':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT6_HUMAN_MEDIATOR']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_HUMAN_MEDIATOR\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  # Candidates can be either model or human statements.\n",
        "  valid_candidate_provenances = (\n",
        "        types.ResponseProvenance.MODEL_MEDIATOR,\n",
        "        types.ResponseProvenance.HUMAN_MEDIATOR, )\n",
        "elif dataset_name == 'virtual_citizens_assembly':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION].isin([\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK3',\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK4',\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK5'\n",
        "  ])]\n",
        "  min_size_parameters = None\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G3Ujd2niS6A"
      },
      "outputs": [],
      "source": [
        "#@title Example pre-processing\n",
        "print('processing', dataset_name)\n",
        "print('original df shape', df.shape)\n",
        "live_loading.check_consistent_tuple_lengths_in_grouped_columns(\n",
        "    df, groups_columns=[\n",
        "        DFGroupedKeys.OTHER_OPINIONS, DFGroupedKeys.CANDIDATES])\n",
        "\n",
        "# First, unnest columns (e.g., ratings of statements).\n",
        "df_unnested = live_loading.unnest_nested_columns(df)\n",
        "print('unnested df shape', df_unnested.shape)\n",
        "\n",
        "# Remove rows where OWN_OPINION is not HUMAN_CITIZEN (e.g., MOCKs).\n",
        "df_unnested = live_loading.filter_on_response_provenances(\n",
        "    df_unnested,\n",
        "    provenance_column=DFKeys.OWN_OPINION_PROVENANCE,\n",
        "    valid_provenances=(types.ResponseProvenance.HUMAN_CITIZEN,),\n",
        ")\n",
        "print('filtered df shape after removing invalid opinions', df_unnested.shape)\n",
        "\n",
        "# Remove rows where CANDIDATES_PROVEANACE is not as expected:\n",
        "# MODEL_MEDIATOR for most data sets. Can also be HUMAN_CITIZEN or HUMAN_MEDIATOR\n",
        "# for opinion exposure and human mediator comparison, respectively.\n",
        "df_unnested = live_loading.filter_on_response_provenances(\n",
        "    df_unnested,\n",
        "    provenance_column=DFKeys.CANDIDATES_PROVENANCE,\n",
        "    valid_provenances=valid_candidate_provenances,\n",
        ")\n",
        "print('filtered df shape after removing invalid candidates', df_unnested.shape)\n",
        "\n",
        "# Remove mock ratings.\n",
        "df_unnested = live_loading.filter_out_mock_ratings(\n",
        "    df_unnested, rating_type=live_loading.RatingTypes.AGREEMENT)\n",
        "print('filtered df shape after removing mock ratings', df_unnested.shape)\n",
        "\n",
        "# Remove mock rankings.\n",
        "df_unnested = live_loading.filter_out_mock_rankings(df_unnested)\n",
        "print('filtered df shape after removing mock rankings', df_unnested.shape)\n",
        "\n",
        "# Add a column with the numerical equivalents for the Likerts.\n",
        "df_unnested = live_loading.add_numerical_ratings(df_unnested)\n",
        "print('added numerical ratings df shape', df_unnested.shape)\n",
        "\n",
        "# Optional (not used in training or human mediator eval):\n",
        "# Remove groups with repeat participants.\n",
        "if remove_groups_with_repeat_participants:\n",
        "  df_unnested = live_loading.filter_groups_with_repeat_participants(\n",
        "      df_unnested, 'worker_id')\n",
        "  print('filtered df after removing groups with repeat participants', df_unnested.shape)\n",
        "\n",
        "# Renest previously unnested columns.\n",
        "df_nested = live_loading.nest_columns_as_tuples(df_unnested)\n",
        "print('renested df shape', df_nested.shape)\n",
        "\n",
        "# Human Mediator specific preprocessing: Only keep rounds where both human and\n",
        "# model generated statements.\n",
        "if dataset_name == 'cohort6_human_mediator':\n",
        "  df_nested = df_nested[\n",
        "      df_nested[DFKeys.CANDIDATES_PROVENANCE].apply(len) == 2\n",
        "  ]\n",
        "  print('only keeping rounds where both human and model made statement',\n",
        "        df_nested.shape)\n",
        "\n",
        "# Optional: Filter by number of groups of min size (pre-registration criteria).\n",
        "# Note, this should be applied to only a single evaluation dataset and not\n",
        "# multiple datasets at the same time.\n",
        "if min_size_parameters is not None:\n",
        "  df_nested = live_loading.filter_by_number_of_groups_of_min_size(\n",
        "      df_nested,\n",
        "      **min_size_parameters.value)\n",
        "  print('filtered df after setting number of groups of min size', df_nested.shape)\n",
        "\n",
        "print('Number of groups in preprocessed dataframe:',\n",
        "      df_nested[DFKeys.LAUNCH_ID].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQW-Dytojh6W"
      },
      "source": [
        "```\n",
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1UWkeUiHhWxYXBZfp5SLYpqlz6p9RK-fp",
          "timestamp": 1729273099487
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
